---
layout: post
author: tom
---

What will be the I/O layer for generative AI? Will it forever be natural language chat? Or is chat just the degenerate case of a much broader tapestry of artificial linguistics?

Large language models (LLMs) try to predict the “optimal” next fragment of a word—called a token—given some cumulative history of a conversation. These models “generate” strings of text by recursively predicting one word after another in sequence. Each new word output by the model gets added to the running conversation, thereby becoming part of the input to the next run of the model.

ChatGPT wowed the world with what next-token prediction can accomplish, given a sufficiently large and well trained language model. Tech companies are tripping over themselves to ride the coattails of these legitimately impressive innovations. This has led to quite a lot of specious—possibly GPT-generated—fuzzy thinking going on about just how applicable LLMs will be to other domains of human endeavor.

What fraction of the work done by humans can realistically be replaced by a chat bot? Quite a few industries will no doubt be disrupted. But the economy is big, and it produces much more than just words.

How might AI actually be usefully applied to more than just words and images? What’s the natural language, token-embedding equivalent for a global supply chain, or a nationwide cell network, or a network of inter-dependent financial institutions? I propose that there is a language, of sorts, that underlies each of these fields—but it’s not English—and LLMs trained on the collective writings of humanity can’t speak it. Thankfully that’s ok, because there are efficient ways for AI to autonomously learn these languages.

Enough beating around the bush. Here’s my claim: the ideal “tokens” for a learnable machine language consist of the state vectors of the machine’s parameter space. That’s a mouthful. Let me break down what I mean, and how it generalizes the techniques of LLMs to supply chains, cell networks, finance, and more.